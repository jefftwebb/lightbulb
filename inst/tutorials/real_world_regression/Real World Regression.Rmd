---
title: "Real World Regression"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(caret)
library(GGally)
library(mice)
library(imputeMissings)
library(missForest)
library(ranger)


homes <- read_csv("old_LAHomes.csv") %>% 
  mutate(city = case_when(city == "Beverly Hills" ~ "San Francisco",
                          city == "Santa Monica" ~ "Menlo Park",
                          city == "Westwood" ~ "Berkeley", TRUE ~ "Oakland"))

set.seed(123)

price <- homes$price
city <- homes$city
#homes$bed <- rpois(1594, 3) + 1
homes$rooms <- homes$bed* 2 + 1 + rbinom(1594, 1, .5)
#homes$bath <- rpois(1594, 2) +2
#homes$pool <- pool
homes <- prodNA(homes, .05)
homes$spa[c(12,945,677)] <- 1
homes$price <- price
homes$city <- city

homes <- homes|>
  select(-garage)|>
  mutate(city = factor(city))

complete_homes <- homes %>% 
  mutate(pool = ifelse(is.na(pool), "N", pool)) %>% 
  dplyr::select(-spa)

complete_homes <- impute(complete_homes, method = "median/mode")

log_mod <- lm(log(price) ~ city + 
             sqft + 
             bed + 
             bath + 
             pool, data = complete_homes)

loglog_mod <- lm(log(price) ~ city + 
             log(sqft) + 
             bed + 
             bath + 
             pool, data = complete_homes)

nolog_mod <- lm(price ~ city + 
             log(sqft) + 
             bed + 
             bath + 
             pool, data = complete_homes)

write_csv(homes, "homes.csv")

```


## Introduction

By "real world regression" I mean to refer to all of the potential complications  associated with using  regression to model actual data.   What issues are you likely to encounter?

- Missing observations.
- Continuous data that is skewed or poorly distributed, with outliers.
- Categorical data with high cardinality (lots of levels in factor variables).
- Non-linear relationships between predictor variables and the target variable.
- High dimensional data---large numbers of predictors as well as observations---that can cause fitting algorithms to bog down.
- Complicated models (lots of coefficients) leading to the possibility of overfitting.
- Many different possible models leading to uncertainty about which one is best.
- Poorly-defined job tasks requiring educated guesses and critical thinking.

The goal in this tutorial is to  equip you with the skills and example code to handle some of the above challenges. 

<!-- What are those skills? -->

<!-- - Removing near zero variance predictors and highly correlated variables. -->
<!-- - Removing highly correlated predictors. -->
<!-- - Log transforming variables. -->
<!-- - Dealing with NAs. -->
<!-- - Assessing outliers. -->
<!-- - Choosing predictors. -->
<!-- - Comparing models using the appropriate performance metrics. -->

<!-- ## Caret Package -->

<!-- The [caret](http://topepo.github.io/caret/index.html) package is an important tool for real world regression.  You were introduced to the package in the previous module.  Here we cover more advanced uses of `caret` to simplify your modeling workflow.  New tools are currently being developed to preprocess data and fit multiple models easily in R, but for now `caret` is among the best tools available for quickly fitting and comparing a wide variety of models. Peruse the large list of methods available in `caret` here:  [available models](http://topepo.github.io/caret/available-models.html). -->

<!-- As we've seen, the workhorse function in `caret` is `train()` which serves as a wrapper for the above methods.  The methods implemented in `train()` are all borrowed from other packages; `caret` merely creates a standardized user interface (UI) for the fitting functions.  Take `lm()` as an example. When we use `train()` to do linear regression, the `lm()` function is doing the work, but the UI is the same as we would use for any other model. This solves a major irritation in the R modeling ecosystem:  idiosyncratic model syntax.   Here are some of the other nice features in `caret`: -->

<!-- - The standardized UI in `caret` makes it easy to rapidly iterate between models to compare performance and do model selection.  Some algorithmic methods will definitely produce better results than others; `caret` allows rapid exploration of the model space to find the best-performing method. -->

<!-- - `caret` automatically handles cross-validation.  In fact, the output printed to the screen  after running the `train()` function supplies metrics on the model's  *estimated out-of-sample  performance*.  This should not be confused with the model's in-sample performance! More on that below. -->

<!-- - The package  also  greatly streamlines and simplifies the pre-processing pipeline for machine learning, making it easy to create test and train sets, make dummy variables, remove near zero variance  and highly correlated predictors, and do imputation for missing data. -->

##  Predicting Home Sale Price

As an example, we will use data similar to what you worked with for the PacDev case, but  exhibiting more real world characteristics, with---among other problems---missing observations and non-linearity.  Here is the data dictionary:  


Each row represents sales information on a single family home in the Bay Area.

- `city` [fcr]: home location.
- `bed` [num]: number of bedrooms.
- `bath` [num]: number of baths.
- `garage` [fcr]: size of garage in cars. 
- `sqft` [num]: size of home.
- `pool` [fcr]: indicator for whether the home has a pool.  
- `spa` [num]: indicator for whether the home has a spa.  
- `rooms` [num]: number of rooms.
- `price` [num]: home sales price. 

To get started, take a look at the data:

```{r x1, exercise = TRUE}
summary(homes) 

```

```{r x2, exercise = TRUE}
glimpse(homes)
```

Three of the variables are factor variables, while the rest are numeric. There is quite a bit of missing data, which we need to address before modeling. 

## Near zero variance predictors

The `caret` package includes a function, `nzv()` (for "near zero variance"), that will identify the columns in a data frame with little or no variation.  

Zero variance predictors contain no information at all---an example would be a constant. By definition, these predictors add nothing to a regression model and should be removed. In fact, the least squares algorithm will not be able to estimate a coefficient for a zero variance predictor. 

Near zero variance predictors contain a little information. Should near zero variance predictors be removed? Not necessarily. In fact, I would recommend not automatically eliminating such variables. A little information is better than none. And including weak predictors in a linear regression model will typically not cause problems.

When might you want to remove near zero variance predictors? Here are some considerations:

- Sometimes you need an excuse to reduce the dimensionality of your data by removing predictors. Either the predictor set is large, which causes the estimation algorithm to bog down, or there are more predictors than instances (rows), in which case an algorithm like OLS regression will not work. Removing predictors is the only option in the latter case, and a good option in the former case.

- Low variance predictors, if they contain a few large outliers, can sometimes produce unrealistic coefficients and standard errors in linear regression.

<!-- Cross-validation procedures will run into trouble with near zero variance predictors because, after dividing the data into folds, some of the predictors will end up with zero variance. When this happens caret will produce a scary warning in red letters: “prediction from a rank-deficient fit may be misleading.” Keep in mind, however, that this is just a warning (at the worst it might affect the reliability of caret’s cross-validation estimates of the model’s out-of-sample performance) and is not by itself as reason to remove predictors. -->

For reference, here is a quick example of how to use `nzv()`:

```{r x3, exercise = TRUE}
nzv(homes, names = T)

```

This result tells us that all the columns in this dataset except `pool` and `spa` have enough variance to function as useful predictors. Here is how you would remove near zero variance variables using `nzv()`: `new_data <- data[, -nzv(data)]`.


## Missing Data

If the missing observations are missing at random (MAR)  or  missing completely at random (MCAR), we could just remove the rows with NAs.  Why?  If the missings are indeed random, then removing them should not alter the general pattern of relationships in the data.  But if the number of missing observations is large, as here, then using only the complete cases can create problems:  we might encounter the problem of not having enough data to create a reliable model, particularly if some of the variables don't have much variation.   

The solution in such situations is to *impute the missing observations*, which means to make an educated guess about the  probable values of the NAs---what they would have been were they not missing. 

Note that NA can mean different things for different datasets.  Read the data dictionary carefully!  Sometimes you don't need to impute NAs but to replace them with appropriate values.  For example, the `Alley` column in the Ames Housing dataset is 90% NAs.  The data dictionary informs us that NA  means the home has no alley. Thus, NA does not represent missing information in this case, but a *missing feature* of the home. In this case we can confidently replace NA with "none." In this dataset, the NAs in `pool` also seem to mean "none" since the only recorded observations are `Y`. And the extent of missing observations in `spa`---1591---combined with the fact that the only recorded values are `1`, means that this variable contains no information. 

```{r x4, exercise = TRUE}
unique(homes$spa)
```

`spa` can be removed without consequence. Let's make these changes now and calculate the complete cases:

```{r x6, exercise = TRUE}
homes <- homes %>% 
  mutate(pool = ifelse(is.na(pool), "N", pool),
         pool = factor(pool)) %>% 
  select(-spa)

sum(complete.cases(homes))
```

The `lm()` function will automatically remove rows with missing observations, so in this case the functional size of the dataset for regression modeling is much smaller than it seems at first. 

How do you know when to impute genuinely missing observations rather than removing the entire row? The short answer is: you don't. You must use your judgment.  If there are just a few missing observations then remove those rows. That is the most convenient thing to do and should not impact results.  But if the missing observations are more than 10-20% of the data set (though this is a *very rough*, ballparky guideline), and if there are large numbers of predictors, and you are including interactions, then you should consider imputation simply in the interest of data preservation. (One caveat is that if the NAs are too extensive then the columns won't contain very much information and imputation won't be very reliable.  In such cases consider starting over with data collection. )

The imputation approach you use will in part depend on your modeling goal. For example, if your goal is inference---that is, if you are most interested in interpreting model coefficients to reason about causal relationships---then you should use *multiple imputation*.  This method creates multiple datasets representing different possibilities for the imputed values. It ensures that the uncertainty inherent in guessing unknown values makes its way appropriately into the standard errors for the model coefficients. If, on the other hand, you need to impute a single value in order to have a complete test set for prediction then you should use *single imputation.*

There are many packages available in R to do imputation.  Most of them are fiddly.  One that is  straightforward to use single imputation is the `imputeMissings` package, which offers several imputation methods, the fastest and simplest of which is imputation with medians (for numeric data) or the mode (for categorical data) using the `impute()` function. 

```{r x7, exercise = TRUE}
complete_homes <- impute(homes, method = "median/mode") 
```

Did it work?

```{r x8, exercise = TRUE}
na.omit(complete_homes) %>% 
  nrow()

```

Yes. That was pretty simple. 

What about multiple imputation? Because the point of multiple imputation is to get accurate standard errors for model coefficients, in the presence of missing data, this method is typically used in conjunction with a fitted model. We will demonstrate with the `mice` package. Here is an example workflow:

```{r include=FALSE}
homes <- homes %>% 
  mutate(pool = ifelse(is.na(pool), "N", pool),
         pool = factor(pool)) %>% 
  select(-spa)
```

```{r warning=FALSE, echo = T}

mice_data <- mice(homes , m = 5,  meth = "rf", seed = 123, printFlag = F)

```

Here are the elements in the above line of code.

- `mice()` is the function used to create mutliple imputed datasets.
- `m` specifies the number of imputed datasets.
- `meth` is the imputation method used; `rf` stands for "random forest."
- `seed` is the random seed for reproducibility.
- `printFlag = F` suppresses verbose output.

We now use the imputed datasets to fit the regression model, as follows:

```{r echo = T}
model <- with(mice_data, lm(price ~ city + bed + bath + sqft + rooms + pool))
summary(pool(model))

```

The `with()` function signals to `mice` to fit multiple models, using the `m` number of imputed datasets, the results of which must then be combined and averaged with `pool()`.  

For convenience going forward we will use the dataset imputed above with single imputation.

However, something seems wrong with this model.  The baseline city is Berkeley.  A simple data summary shows that homes in San Francisco should not be worth less. The sign on the coefficient for San Francisco seems wrong.

```{r x3_1, exercise = TRUE}
complete_homes %>%
  group_by(city) %>%
  summarize(avg_price = mean(price)) %>%
  arrange(avg_price)
```

What is going wrong?

<!-- using `pool()`which then must be  -->
<!-- This package implements what is known as "single imputation"  because it produces just a single complete data set. Single imputation is what we need when doing prediction.  (You will also see references to "multiple imputation," which is the go-to method for inference but won't work---at least not as designed---for prediction.)  `missForest` works well for modest amounts of imputation, but, unfortunately, is extremely time-consuming for large data sets. The upside of this package---that it does multivariable imputation, using information from all the other variables, and is therefore arguably more accurate than alternatives---is also its downside: it is slow, sometimes impossibly so.  Below is an example for how to use missForest. -->

<!-- What are the alternatives?  *Median imputation.* `caret`  will conveniently do median imputation within the `train()` function, while fitting a model. (It will do other sorts of imputation as well, but these also suffer from the problem mentioned above:  slowness.) Median imputation is a good choice when there are many missing observations and the number of predictors is large. However, the price we pay for speed is---perhaps---accuracy:  the method uses no information from other variables but instead simply uses the median in any given column to replace missing values.  Why use the median and not the mean?  The median is less sensitive to outliers, and should be very close to the mean when there are no outliers.   -->

<!-- Imputing missing data is a step in what we will call the *modeling workflow* or *modeling pipeline*.   -->

<!-- ## Modeling Pipeline with `caret` -->

<!-- Following an order in pre-processing and modeling steps can make a huge difference in efficiency. Here is a possible workflow.  Note that some of these steps, depending on the modeling scenario, are optional. -->

<!-- 1. Clean the data. As we've seen, this entails, at a minimum, removing data that is logically impossible or obviously the result of a data collection error. -->
<!-- 2. Make all predictors numeric.  This is an optional step that involves  dummy coding the categorical  variables.  -->
<!-- 3. Consider removing predictors that contain little information, known as zero variance or near zero variance variables. This is an optional step. -->
<!-- 4. Consider removing highly correlated variables. This is an optional step that depends on your modeling objective.  Highly correlated predictors---known as multicollinearity---can make it difficult to use a model for inference, since correlated predictors will tend to have inflated standard errors.  Note that multicollnearity should not affect predictive accuracy. -->
<!-- 5. Deal with missing observations. If using the `caret` package, imputation can be done while fitting the model. -->
<!-- 6. Fit multiple models and compare estimated out-of-sample error metrics.  -->


## Multicolinearity

Highly correlated predictors add noise to a regression model without adding information, and can cause instability in standard error and coefficient estimates. This  is obviously unwelcome if your goal is inference.  Note that *multicollinearity is not a problem for prediction*.

There are more or less complicated ways of identifying multicollinearity statistically.  Various functions in R will use what is known as the "variance inflation factor" or VIF to identify correlated predictors for removal.  More simply, bivariate correlations can be calculated between all the predictors and then one of each correlated pair above a certain threshold can be removed. (See [Chapter 3.3](http://topepo.github.io/caret/pre-processing.html#identifying-correlated-predictor) in the caret documentation for information on how to implement this strategy.) However you identify the correlated predictors, the solution is to remove them.  

As noted, the symptoms of multicolinearity include instability in standard errors and coefficients.  You will see these estimates go haywire when the correlated predictor is added to the model.  Witness:

```{r x9, exercise = TRUE}
lm(price ~ rooms, data = complete_homes) %>% 
  summary
```

Observe what happens to the coefficient, SE and p-value for `rooms` when we add `beds`:

```{r x10, exercise = TRUE}
lm(price ~ rooms + bed, data = complete_homes) %>% 
  summary
```

This behavior is due to the correlation between `bed` and `rooms`:

```{r x11, exercise = TRUE}
with(complete_homes, cor(bed, rooms))
```

Clearly, these two variables contain virtually the same information. To reason accurately about effect sizes and statistical significance we need to remove one or the other.  For purposes of prediction, however, it would be fine to leave both in the model, since together they contain more information than just one alone.


## Non-linear Data

Price data is typically non-linear.  We see that in this dataset:

```{r x12, exercise = TRUE}

ggplot(complete_homes, aes(sqft, price)) +
  geom_point() +
  theme_minimal() +
  stat_smooth(method="lm", se = F) +
  labs(title = "price ~ sqft")

```

The `price` variable spans many orders of magnitude, and the relationship between `price` and `sqft` is not linear, *especially* in San Francisco.

```{r x13, exercise = TRUE}

ggplot(complete_homes, aes(sqft, price)) +
  geom_point() +
  theme_minimal() +
  facet_wrap(~city) +
  stat_smooth(method="lm", se = F) +
  labs(title = "price ~ sqft, varying by city")

```

A linear model will have trouble with non-linear data.  

### Log Transformation

One solution is to log transform  some of the variables.  Log transformation compresses right skewed data and helps the linear model fit better.   The function we often use in R for log transformation, `log()`, uses the natural log.  (`log2()` will compute log base 2.) After log transformation of price, each fixed distance in price represents a multiplication (not an addition) of the value.

```{r x14, exercise = TRUE}

ggplot(complete_homes, aes(sqft, log(price))) +
  geom_point() +
  theme_minimal() +
  stat_smooth(method="lm", se = F) +
  labs(title = "log(price) ~ sqft")

```

That doesn't look like an improvement. Remember that our goal in transforming the data is to make the relationship between two variables linear, so as to improve the fit of a linear model.  The problem is that `sqft` is also right skewed. So, let's log transform `sqft` as well.  


```{r x15, exercise = TRUE}
ggplot(complete_homes, aes(log(sqft), log(price))) +
  geom_point() +
  theme_minimal() +
  stat_smooth(method="lm", se = F) +
  labs(title = "log(price) ~ sqft")
```

That looks much better!

What is a logarithm anyway?  Think of the Richter scale for earthquake magnitude. Each linear increment---3 to 4, for example---represents a multiplicative  increase in size: 10 times larger.  This is because the Richter scale uses the log base 10 scale.  Basically: $10^3$ = 10 x 10 x 10 = 1000 vs. $10^4$ = 10 x 10 x 10 x 10 = 10,000, which is 10 times larger than $10^3$. The linear increments of the Richter scale uses the exponents on 10, in this case 3 vs. 4.

In statistics we almost always use the natural log, where the base is not 10 but the mathematical constant $e$.  What is $e$? The value of $e$ is $e^1$.

```{r x16, exercise = TRUE}
(e <- exp(1)) 
```

The natural log asks:  to what power must $e$ (2.718...) be raised to equal another number? So if we take the natural log of, say, 9, we are asking: to what power must $e$ be raised to equal 9? It must be raised to the power of 2.197...

```{r x17, exercise = TRUE}
exp(1) ^ 2.197225 
```

How do we know that?

```{r x18, exercise = TRUE}
log(9)
```

Key fact: Natural log undoes exponentiation (raising $e$ to a power) and exponentiation undoes natural log.  Use this identity: $e^{ln(x)}$ = x.  In R: `exp(log(x))` = x. Observe:

```{r x19, exercise = TRUE}
exp(log(9))
```

```{r x20, exercise = TRUE}

log(exp(9))

```

Here is what the natural log function looks like:

```{r x21, exercise = TRUE}
data.frame(x = seq(1, 1000, by = 1)) %>% 
  mutate(natural_log_x = log(x)) %>% 
  ggplot(aes(x, natural_log_x)) +
  geom_line()
```

Notice that the natural log values are compressed: the range 0-1000 becomes 0-6.5. This is important because a natural log transformation will help make outliers less influential in a regression. 

And here is the reverse of the natural log, the exponential function:

```{r x22, exercise = TRUE}
data.frame(x = seq(1, 10, by = .01)) %>% 
  mutate(exp_x = exp(x)) %>% 
  ggplot(aes(x, exp_x)) +
  geom_line()
```

A log transformation takes an exponential increase and makes it linear, which the the goal in linear modeling. 

```{r x23, exercise = TRUE}
data.frame(x = seq(1, 10, by = .01)) %>% 
  mutate(exp_x = exp(x)) %>% 
  ggplot(aes(x, log(exp_x))) +
  geom_line()
```

Such a transformation will often improve model fit. Why? Think about it this way:  

- Your goal in creating a model is to faithfully summarize the data, but outlying observations can frustrate that goal by unduly influencing the model.  Consider a simple data summary like a mean.  If the data are skewed, the mean will be pulled away from the center of the data and will not be very representative. The same thing happens in a linear model when the outcome variable is skewed.

- If we can make outliers less anomalous with a data transformation then we can reduce their influence and improve the model.  That is the point of log transforming the outcome variable. 

Will log transforming `price` improve the model? We can answer this question empirically by comparing models using $R^2$. In the models below I will use shorthand: `.`  means to include all predictors.

No log transformation:

```{r x24, exercise = TRUE}
summary(nolog_mod <- lm(price ~ ., data = complete_homes))$r.square
```

```{r x25, exercise = TRUE}
plot(nolog_mod, which = 1)
```
Terrible!  The exponential increase in `price` shows up in the residuals.

Log transform `price` (a log model):

```{r x26, exercise = TRUE}
summary(log_mod <- lm(log(price) ~ ., data = complete_homes))$r.square
```

```{r x27, exercise = TRUE}
plot(log_mod, which = 1)
```


Log transform `price` and `sqft` (a log-log model):

```{r x28, exercise = TRUE}
summary(loglog_mod <- lm(log(price) ~ . -sqft + log(sqft), data = complete_homes))$r.square
```

The shorthand code here says to first include all predictors, then take out `sqft` and add in `log(sqft)`.

```{r x29, exercise = TRUE}
plot(loglog_mod, which = 1)
```


The third model, judged by $R^2$, and the residual plot, is the best.

One thing to keep in mind is that after log transformation the model's fitted values will be expressed in log units.  This won't affect $R^2$, since it is calculated from a ratio---$1 -\frac{RSS}{TSS}$---and  therefore does not depend on the underlying units. But RMSE will be affected.  To calculate RMSE from a log or log-log model in original units (rather than log units)  will require exponentiating the fitted values:  $ \sqrt{\frac{1}{n}(y_i - e^{\hat{y}_i})^2}$.

Here is residual standard deviation, in log units, for the log-log model:

```{r x30, exercise = TRUE}
summary(loglog_mod)$sigma
```

And here is the calculation of RMSE in original units:

```{r x31, exercise = TRUE}

rmse <- function(actual, fitted) sqrt(mean(actual - fitted)^2)


rmse(complete_homes$price, exp(fitted(loglog_mod)))
```

You can see the advantage of computing RMSE in dollars rather than log dollars.  The average error in the log-log model is meaningless, except for model comparison.  But having the error in dollars provides a clear sense of the model's accuracy.

### Interpreting a log model

The coefficients in a log or log-log model are expressed in log units, which are hard to connect to real world quantities.  

It is always possible to leave the coefficients expressed in log units and interpret them as for a model with an unlogged outcome. For example, the coefficient for log `sqft` could be interpreted as the change in log `price` associated with one unit increase in log `sqft`, holding the other predictors constant.  This provides limited insight, however, since log units are not intuitive. So, how would we make the scale of these coefficients easier to understand?


For reference, here are the coefficients for the log model.

```{r x32, exercise = TRUE}
log_mod$coefficients
```

We need to exponentiate coefficients in a log model to get the percentage increase in the outcome associated with a 1 unit increase in the predictor, holding the other predictors constant. Thus, the coefficient for Menlo Park, .317, would `exp(.317)` = 1.373, which is a 37.3% increase compared the baseline of 1: 1.373 - 1  = -.373 * 100 = 37.3%. In other words, we see a 37.3% increase in average home prices in Menlo Park compared to the reference city of Berkeley, everything being equal.  As a rule of thumb, we can dispense with exponentiation when the coefficient is close to 1 because exponentiating returns a number very close to the original. For example, `exp(.02)` = 1.02, representing a 2% increase over the baseline of 1.  (The calculation goes like this: 1.02 - 1 = .02 * 100 = 2%.) In this example the rule of thumb works well. But it does not work well for the Menlo Park coefficient which is further from 0, as we saw above.


For reference, here are the coefficients for the log-log model.


```{r x33, exercise = TRUE}
loglog_mod$coefficients
```


How would coefficients in a log-log model, in which *both* the outcome and a predictor had been log transformed, be interpreted?  Mercifully, we don't need to exponentiate.  Instead, we can interpret the coefficient directly as a percentage increase.  The coefficient for `log(sqft)` is  .976.  This would mean that each 1% increase in square feet is associated with a .976% increase in the price. The other coefficients would be interpreted similarly as percentage increases.

Notice that the signs on the coefficients in this model now make sense, given what we know about average home prices: Oakland has lower prices than Berkeley, but San Francisco and Menlo Park have higher prices. 

## Outliers

There are some extreme home prices in this dataset:

```{r x34, exercise = TRUE}
homes %>% 
  arrange(desc(price)) %>% 
  slice(1)
```

This is an outlier, yes, but it is not---presumably---a data collection error, but simply a very large (and expensive) home.

In general, outliers *should not* be reflexively removed from a data set. Simply because the model fit is better without an outlier is not a reason to remove it!  Instead:

1. Ask questions about data collection.  Is the outlier a mistake, the result of an error?  

2. Ask questions of domain experts  to understand other factors influencing home sales.  Could a variable be added, or created, that would help explain high prices? 


